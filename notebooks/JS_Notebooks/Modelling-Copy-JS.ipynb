{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f9d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Turn off all warnings (not recommended unless you're sure)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d172aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053bb6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joshstone/code/lucasglanville/and_theyre_off_backend\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45900651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data acquired with shape (118575, 116)\n",
      "first 221 rows removed. New shape = (118354, 116)\n",
      "Cleaned up missing odds. New shape = (118093, 116)\n",
      "Added Josh features. New shape = (118092, 123)\n",
      "Added Oli features 2/4. New shape = (118092, 125)\n",
      "Added Oli features 4/4. New shape = (118092, 127)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#from params import *\n",
    "\n",
    "\n",
    "def get_data(LOCAL_FILEPATH):\n",
    "    data = pd.read_csv(LOCAL_FILEPATH)\n",
    "    print(f\"data acquired with shape {data.shape}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#This is the first 2 days worth of data where general_runs stats are all skewed\n",
    "def remove_221_rows(data):\n",
    "    data = data[221:]\n",
    "    print(f\"first 221 rows removed. New shape = {data.shape}\")\n",
    "    return data\n",
    "\n",
    "## Dropping rows if we have no betting data, and imputing if we have some betting data\n",
    "def dropping_no_betting_data(data):\n",
    "    data = data.dropna(subset=['f_bsp', 'f_pm_15m', 'f_pm_10m', 'f_pm_05m' , 'f_pm_03m', 'f_pm_02m', 'f_pm_01m'], how='all')\n",
    "    def impute_row(row):\n",
    "        row = row.fillna(method='ffill').fillna(method='bfill')\n",
    "        return row\n",
    "    columns_to_impute = ['f_bsp', 'f_pm_15m', 'f_pm_10m', 'f_pm_05m', 'f_pm_03m', 'f_pm_02m', 'f_pm_01m']\n",
    "    data[columns_to_impute] = data[columns_to_impute].apply(impute_row, axis=1)\n",
    "\n",
    "    #deleting row that has 0s for all odds for some reason\n",
    "\n",
    "\n",
    "    print(f\"Cleaned up missing odds. New shape = {data.shape}\")\n",
    "    return data\n",
    "\n",
    "def josh_features(data):\n",
    "\n",
    "    #Creating country feature\n",
    "    irish_tracks = [\n",
    "    \"SLIGO\", \"LIMERICK\", \"NAVAN\", \"WEXFORD\", \"CURRAGH\",\n",
    "    \"GALWAY\", \"KILBEGGAN\", \"GOWRAN PARK\", \"BELLEWSTOWN\",\n",
    "    \"LISTOWEL\", \"THURLES\", \"BALLINROBE\", \"TRAMORE\",\n",
    "    \"LEOPARDSTOWN\", \"DOWN ROYAL\", \"ROSCOMMON\", \"CORK\",\n",
    "    \"DUNDALK\", \"KILLARNEY\", \"LAYTOWN\", \"TIPPERARY\",\n",
    "    \"FAIRYHOUSE\", \"NAAS\", \"DOWNPATRICK\", \"CLONMEL\",\n",
    "    \"PUNCHESTOWN\"\n",
    "]\n",
    "\n",
    "    data['country'] = data['f_track'].apply(lambda x: 'IRE' if x in irish_tracks else 'GB')\n",
    "\n",
    "    #Completing f_class for Irish races\n",
    "\n",
    "    # Calculate mean ratings for each 'f_id' group\n",
    "    mean_ratings_by_id = data.groupby('f_id')['f_rating_or'].mean()\n",
    "\n",
    "    # Define the mapping of mean ratings to f_class values\n",
    "    rating_to_f_class_mapping = {\n",
    "        (96, float('inf')): 1,\n",
    "        (86, 96): 2,\n",
    "        (76, 86): 3,\n",
    "        (66, 76): 4,\n",
    "        (56, 66): 5,\n",
    "        (46, 56): 6,\n",
    "        (-float('inf'), 46): 7\n",
    "    }\n",
    "\n",
    "    # Function to map mean ratings to f_class values\n",
    "    def map_rating_to_f_class(mean_rating):\n",
    "        for rating_range, f_class_value in rating_to_f_class_mapping.items():\n",
    "            if rating_range[0] <= mean_rating <= rating_range[1]:\n",
    "                return f_class_value\n",
    "\n",
    "    # Apply the mapping to fill NULL values in 'f_class' column based on mean ratings\n",
    "    data['f_class'] = data.apply(lambda row: map_rating_to_f_class(mean_ratings_by_id.get(row['f_id'])), axis=1)\n",
    "\n",
    "    # Now the 'f_class' column should be filled based on the specified mapping using mean ratings\n",
    "\n",
    "    # Merge the mean ratings back into the original DataFrame based on 'f_id'\n",
    "    data = data.merge(mean_ratings_by_id, how='left', left_on='f_id', right_index=True)\n",
    "\n",
    "    # Rename the merged mean rating column for clarity\n",
    "    data.rename(columns={'f_rating_or_y': 'mean_f_rating_or_race', 'f_rating_or_x' : 'f_rating_or' }, inplace=True)\n",
    "\n",
    "    # Create official rating vs average rating in the race feature\n",
    "    data['or_rating_vs_avg_race'] = data['f_rating_or'] - data['mean_f_rating_or_race']\n",
    "\n",
    "    # Create odds percentage and movement features\n",
    "    data['15m_odds_prob'] = 1 / data['f_pm_15m']\n",
    "    data['5m_odds_prob'] = 1 / data['f_pm_05m']\n",
    "    data['15to5m_odds_move_perc'] = (data['5m_odds_prob'] / data['15m_odds_prob'] - 1)\n",
    "    data['15to5m_odds_move_raw'] = (data['5m_odds_prob'] - data['15m_odds_prob'])\n",
    "    print(f\"Added Josh features. New shape = {data.shape}\")\n",
    "    return data\n",
    "\n",
    "def class_or_rating_average(data):\n",
    "    average_or_rating_class = data.groupby(\"f_class\")['f_rating_or'].mean().reset_index()\n",
    "    average_or_rating_class.rename(columns={'f_rating_or': 'average_or_rating_class'}, inplace=True)\n",
    "    data = data.merge(average_or_rating_class, on='f_class')\n",
    "    data['above_below_official_rating_class'] = data['f_rating_or']- data['average_or_rating_class']\n",
    "    print(f\"Added Oli features 2/4. New shape = {data.shape}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def oli_features(data):\n",
    "    data = data.sort_values(by='f_ko')\n",
    "    data['PreviousPosition'] = data.groupby('f_horse')['f_place'].shift(fill_value=0)\n",
    "\n",
    "    data = data.sort_values(by=['f_id', 'pred_isp'])\n",
    "    data['PredictedRank'] = data.groupby('f_id').cumcount() + 1\n",
    "    print(f\"Added Oli features 4/4. New shape = {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Define a lambda function to apply the conditions and fill NaN values\n",
    "def fill_f_pm_01m(data):\n",
    "    def fill_nan(row):\n",
    "        if row['f_place'] == 0:\n",
    "            return -1\n",
    "        elif row['f_place'] == 1:\n",
    "            return (row['f_pm_01m'] - 1) * 0.95\n",
    "        else:\n",
    "            return row['f_pm_01m_p_back']\n",
    "\n",
    "# Apply the lambda function to fill NaN values in f_pm_01m_p_back\n",
    "    data['linear_target'] = data.apply(fill_nan, axis=1)\n",
    "    return data\n",
    "\n",
    "data = get_data(\"raw_data/raw_data_v2.2.csv\")\n",
    "data = remove_221_rows(data)\n",
    "data = dropping_no_betting_data(data)\n",
    "    #Dropping line that has 0 for all odds\n",
    "data = data[data.id != 16157910000382]\n",
    "data = josh_features(data)\n",
    "data = class_or_rating_average(data)\n",
    "data = oli_features(data)\n",
    "data.to_csv(\"raw_data/data_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37997ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37133ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_v2(data: pd.DataFrame) -> np.ndarray:\n",
    "    #--------------------------------------------------------------------------#\n",
    "    print('number of columns: ', len(data.columns))\n",
    "    ### DROP IRRELEVANT COLUMNS ###\n",
    "    redundant_columns = [\n",
    "                         #\"id\",\n",
    "                         #\"f_id\",\n",
    "                         \"f_racetype\",\n",
    "                         #\"f_horse\",\n",
    "                         \"f_jockey\",\n",
    "                         \"f_trainer\",\n",
    "                         \"f_rating_hc\",\n",
    "                         \"f_lto_pos\",\n",
    "                         \"f_bsp\",\n",
    "                         #\"f_pm_15m\",\n",
    "                         \"f_pm_10m\",\n",
    "                         \"f_pm_05m\",\n",
    "                         \"f_pm_03m\",\n",
    "                         \"f_pm_02m\",\n",
    "                         #\"f_pm_01m\",\n",
    "                         \"f_bsp_p_back\",\n",
    "                         \"f_bsp_p_lay\",\n",
    "                         #\"f_pm_01m_p_back\",\n",
    "                         \"f_pm_01m_p_lay\",\n",
    "                         #\"f_pm_15m_p_back\",\n",
    "                         \"f_pm_15m_p_lay\",\n",
    "                         \"general_runs_win_at\",\n",
    "                         \"general_runs_win_l200r\",\n",
    "                         \"general_runs_win_l50r\",\n",
    "                         \"general_runs_win_l16r\",\n",
    "                         \"general_runs_at\",\n",
    "                         \"general_runs_l200r\",\n",
    "                         \"general_runs_l50r\",\n",
    "                         \"general_runs_l16r\",\n",
    "                         \"sum_bsp_trainer_at\",\n",
    "                         \"sum_bsp_jockey_at\",\n",
    "                         \"sum_bsp_horse_at\",\n",
    "                         \"sum_bsp_trainer_l16r\",\n",
    "                         \"sum_bsp_jockey_l16r\",\n",
    "                         \"sum_bsp_trainer_l50r\",\n",
    "                         \"sum_bsp_jockey_l50r\",\n",
    "                         \"sum_bsp_trainer_l200r\",\n",
    "                         \"sum_bsp_jockey_l200r\",\n",
    "                         \"sum_bsp_horse_l10r\",\n",
    "                         \"sum_bsp_horse_l5r\",\n",
    "                         \"sum_bsp_horse_l2r\",\n",
    "                         \"15to5m_odds_move_perc\",\n",
    "                         \"15to5m_odds_move_raw\",\n",
    "                         \"15m_odds_prob\",\n",
    "                         \"5m_odds_prob\"]\n",
    "    data.drop(columns = redundant_columns, inplace = True)\n",
    "    print(\"✅ DROPPED IRRELEVANT COLUMNS\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### DROP ROWS WITH NULL VALUES IN THESE COLUMNS ###\n",
    "    data.dropna(axis = 0, inplace = True,\n",
    "                subset = [\"f_pace\",\n",
    "                          \"f_stall\",\n",
    "                          \"stall_position\",\n",
    "                          \"f_going\",\n",
    "                          \"f_rating_or\",\n",
    "                          \"or_rating_vs_avg_race\",\n",
    "                          \"country\",\n",
    "                          \"mean_f_rating_or_race\",\n",
    "                          \"f_class\",\n",
    "                          \"average_or_rating_class\",\n",
    "                          \"above_below_official_rating_class\",\n",
    "                          \"PreviousPosition\",\n",
    "                          \"PredictedRank\",\n",
    "                          ])\n",
    "    print(\"✅ DROPPED ROWS WITH NULL VALUES\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### STRIP SURROUNDING WHITESPACE ###\n",
    "    data['f_track'] = data['f_track'].str.strip()\n",
    "    print(\"✅ WHITESPACE STRIPPED FROM 'f_track'\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### CONVERT ODDS TO PROBABILITY ###\n",
    "    def odds_to_prob(x):\n",
    "        return 1/x\n",
    "    data['pred_isp'] = data['pred_isp'].apply(odds_to_prob)\n",
    "    print(\"✅ ODDS CONVERTED TO PROBABILITY (1/ODDS)\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### CODE WINNERS AS '1', REST AS '0' ###\n",
    "    def winner(x):\n",
    "        if x == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    data['f_place'] = data['f_place'].apply(winner)\n",
    "    print(\"✅ WINNERS CODED AS '1', REST '0'\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### 'f_ko' CONVERTED TO DATETIME ###\n",
    "    data['f_ko'] = data['f_ko'].astype('datetime64[ns]')\n",
    "    print(\"✅ 'f_ko' CONVERTED TO DATETIME\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### ENCODE THE GOING (GROUND CONDITION) ###\n",
    "    def f_going_coder(x):\n",
    "        if x == 'FRM':\n",
    "            return 1\n",
    "        elif x == 'GTF':\n",
    "            return 2\n",
    "        elif x == 'GD' or x == 'GTY' or x == 'STD':\n",
    "            return 3\n",
    "        elif x == 'YLD' or x == 'YTS' or x == 'STSL' or x == 'GTS':\n",
    "            return 4\n",
    "        elif x == 'SFT':\n",
    "            return 5\n",
    "        elif x == 'HVY' or x == 'HTS':\n",
    "            return 6\n",
    "    data['f_going'] = data['f_going'].apply(f_going_coder)\n",
    "    print(\"✅ TRACK CONDITIONS ORDINALLY ENCODED\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### MINMAX SCALE NUMERIC FEATURES ###\n",
    "    set_config(transform_output = \"pandas\")\n",
    "    numeric_features = [\"f_going\",\n",
    "                        \"average_or_rating_class\",\n",
    "                        \"above_below_official_rating_class\",\n",
    "                        \"PreviousPosition\",\n",
    "                        \"PredictedRank\",\n",
    "                        \"f_distance\",\n",
    "                        \"f_class\",\n",
    "                        \"f_age\",\n",
    "                        \"f_pace\",\n",
    "                        \"f_weight\",\n",
    "                        \"f_runners\",\n",
    "                        \"f_rating_or\",\n",
    "                        \"mean_f_rating_or_race\",\n",
    "                        \"or_rating_vs_avg_race\",\n",
    "                        \"f_rating_rbd\",\n",
    "                        \"f_stall\",\n",
    "                        \"stall_position\",\n",
    "                        \"trainer_runs_win_at\",\n",
    "                        \"trainer_runs_win_l200r\",\n",
    "                        \"trainer_runs_win_l50r\",\n",
    "                        \"trainer_runs_win_l16r\",\n",
    "                        \"trainer_runs_at\",\n",
    "                        \"trainer_runs_l200r\",\n",
    "                        \"trainer_runs_l50r\",\n",
    "                        \"trainer_runs_l16r\",\n",
    "                        \"jockey_runs_win_at\",\n",
    "                        \"jockey_runs_win_l200r\",\n",
    "                        \"jockey_runs_win_l50r\",\n",
    "                        \"jockey_runs_win_l16r\",\n",
    "                        \"jockey_runs_at\",\n",
    "                        \"jockey_runs_l200r\",\n",
    "                        \"jockey_runs_l50r\",\n",
    "                        \"jockey_runs_l16r\",\n",
    "                        \"horse_runs_win_at\",\n",
    "                        \"horse_runs_win_l10r\",\n",
    "                        \"horse_runs_win_l5r\",\n",
    "                        \"horse_runs_win_l2r\",\n",
    "                        \"horse_runs_at\",\n",
    "                        \"horse_runs_l10r\",\n",
    "                        \"horse_runs_l5r\",\n",
    "                        \"horse_runs_l2r\",\n",
    "                        \"iv_horse_at\",\n",
    "                        \"iv_trainer_l200r\",\n",
    "                        \"iv_trainer_l50r\",\n",
    "                        \"iv_trainer_l16r\",\n",
    "                        \"iv_trainer_at\",\n",
    "                        \"iv_jockey_l200r\",\n",
    "                        \"iv_jockey_l50r\",\n",
    "                        \"iv_jockey_l16r\",\n",
    "                        \"iv_jockey_at\",\n",
    "                        \"ae_horse_l10r\",\n",
    "                        \"ae_horse_l5r\",\n",
    "                        \"ae_horse_l2r\",\n",
    "                        \"ae_horse_at\",\n",
    "                        \"ae_trainer_l200r\",\n",
    "                        \"ae_trainer_l50r\",\n",
    "                        \"ae_trainer_l16r\",\n",
    "                        \"ae_trainer_at\",\n",
    "                        \"ae_jockey_l200r\",\n",
    "                        \"ae_jockey_l50r\",\n",
    "                        \"ae_jockey_l16r\",\n",
    "                        \"ae_jockey_at\",\n",
    "                        \"rolling_avg_trainer_finish_at\",\n",
    "                        \"rolling_avg_trainer_finish_l200r\",\n",
    "                        \"rolling_avg_trainer_finish_l50r\",\n",
    "                        \"rolling_avg_trainer_finish_l16r\",\n",
    "                        \"rolling_avg_horse_finish_at\",\n",
    "                        \"rolling_avg_horse_finish_l10r\",\n",
    "                        \"rolling_avg_horse_finish_l5r\",\n",
    "                        \"rolling_avg_horse_finish_l2r\",\n",
    "                        \"rolling_avg_jockey_finish_at\",\n",
    "                        \"rolling_avg_jockey_finish_l200r\",\n",
    "                        \"rolling_avg_jockey_finish_l50r\",\n",
    "                        \"rolling_avg_jockey_finish_l16r\",\n",
    "                        ]\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"scaler\", MinMaxScaler())])\n",
    "    print(\"✅ NUMERIC FEATURES MINMAX-SCALED\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### IMPUTE HEADGEAR NULLS WITH 'no_headgear' ###\n",
    "    headgear_feature = [\"f_headgear\"]\n",
    "    headgear_imputer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy = 'constant',\n",
    "                                         fill_value = 'no_headgear'))])\n",
    "    print(\"✅ IMPUTED 'no_headgear' for NULLS IN 'f_headgear'\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### MEAN IMPUTE CERTAIN FEATURES ###\n",
    "    mean_impute_features = [\"f_dob\", \"f_prb_avg\"]\n",
    "    mean_imputer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy = 'mean'))])\n",
    "    print(\"✅ IMPUTED MEAN FOR NULLS IN 'f_dob' & 'f_prb_avg'\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### IMPUTE NULLS WITH 0's ###\n",
    "    zero_impute_features = [\"f_rating_rbd\",\n",
    "                            \"trainer_runs_win_at\",\n",
    "                            \"trainer_runs_win_l200r\",\n",
    "                            \"trainer_runs_win_l50r\",\n",
    "                            \"trainer_runs_win_l16r\",\n",
    "                            \"trainer_runs_at\",\n",
    "                            \"trainer_runs_l200r\",\n",
    "                            \"trainer_runs_l50r\",\n",
    "                            \"trainer_runs_l16r\",\n",
    "                            \"jockey_runs_win_at\",\n",
    "                            \"jockey_runs_win_l200r\",\n",
    "                            \"jockey_runs_win_l50r\",\n",
    "                            \"jockey_runs_win_l16r\",\n",
    "                            \"jockey_runs_at\",\n",
    "                            \"jockey_runs_l200r\",\n",
    "                            \"jockey_runs_l50r\",\n",
    "                            \"jockey_runs_l16r\",\n",
    "                            \"horse_runs_win_at\",\n",
    "                            \"horse_runs_win_l10r\",\n",
    "                            \"horse_runs_win_l5r\",\n",
    "                            \"horse_runs_win_l2r\",\n",
    "                            \"horse_runs_at\",\n",
    "                            \"horse_runs_l10r\",\n",
    "                            \"horse_runs_l5r\",\n",
    "                            \"horse_runs_l2r\",\n",
    "                            \"iv_horse_at\",\n",
    "                            \"iv_trainer_l200r\",\n",
    "                            \"iv_trainer_l50r\",\n",
    "                            \"iv_trainer_l16r\",\n",
    "                            \"iv_trainer_at\",\n",
    "                            \"iv_jockey_l200r\",\n",
    "                            \"iv_jockey_l50r\",\n",
    "                            \"iv_jockey_l16r\",\n",
    "                            \"iv_jockey_at\",\n",
    "                            \"ae_horse_l10r\",\n",
    "                            \"ae_horse_l5r\",\n",
    "                            \"ae_horse_l2r\",\n",
    "                            \"ae_horse_at\",\n",
    "                            \"ae_trainer_l200r\",\n",
    "                            \"ae_trainer_l50r\",\n",
    "                            \"ae_trainer_l16r\",\n",
    "                            \"ae_trainer_at\",\n",
    "                            \"ae_jockey_l200r\",\n",
    "                            \"ae_jockey_l50r\",\n",
    "                            \"ae_jockey_l16r\",\n",
    "                            \"ae_jockey_at\",\n",
    "                            \"rolling_avg_trainer_finish_at\",\n",
    "                            \"rolling_avg_trainer_finish_l200r\",\n",
    "                            \"rolling_avg_trainer_finish_l50r\",\n",
    "                            \"rolling_avg_trainer_finish_l16r\",\n",
    "                            \"rolling_avg_horse_finish_at\",\n",
    "                            \"rolling_avg_horse_finish_l10r\",\n",
    "                            \"rolling_avg_horse_finish_l5r\",\n",
    "                            \"rolling_avg_horse_finish_l2r\",\n",
    "                            \"rolling_avg_jockey_finish_at\",\n",
    "                            \"rolling_avg_jockey_finish_l200r\",\n",
    "                            \"rolling_avg_jockey_finish_l50r\",\n",
    "                            \"rolling_avg_jockey_finish_l16r\"]\n",
    "    zero_imputer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy = 'constant',\n",
    "                                         fill_value = 0))])\n",
    "    print(\"✅ IMPUTED '0' FOR NULLS IN 68 x FEATURES\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### O.H.E. CATEGORICAL FEATURES ###\n",
    "    categorical_features = [\"f_track\", \"f_headgear\", \"country\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\",\n",
    "                                     sparse_output=False))])\n",
    "    print(\"✅ CAT. FEATURES OH-ENCODED (Track, Headgear, Country)\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### COLUMN TRANSFORMER ###\n",
    "    ct1 = ColumnTransformer(\n",
    "        transformers=[(\"zero_imputer\", zero_imputer, zero_impute_features),\n",
    "                      (\"headgear_imputer\", headgear_imputer, headgear_feature),\n",
    "                      (\"mean_imputer\", mean_imputer, mean_impute_features),\n",
    "                      ],\n",
    "        verbose_feature_names_out = False,\n",
    "        remainder = 'passthrough')\n",
    "\n",
    "    ct1_processed = ct1.fit_transform(data)\n",
    "\n",
    "    print('number of columns: ', len(ct1_processed.columns))\n",
    "\n",
    "    ct2 = ColumnTransformer(\n",
    "        transformers=[(\"cat\", categorical_transformer, categorical_features)],\n",
    "        verbose_feature_names_out = False,\n",
    "        remainder = 'passthrough')\n",
    "\n",
    "    ct2_processed = ct2.fit_transform(ct1_processed)\n",
    "\n",
    "    ct3 = ColumnTransformer(\n",
    "        transformers=[(\"scale\", numeric_transformer, numeric_features),],\n",
    "        verbose_feature_names_out = False,\n",
    "        remainder = 'passthrough')\n",
    "\n",
    "    print(\"✅ COLUMN TRANSFORMER ASSEMBLED\")\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    ### FIT_TRANSFORM FEATURES ###\n",
    "    print(\"⏳ FIT_TRANSFORMING THE PREPROCESSING PIPE...\")\n",
    "    data_processed = ct3.fit_transform(ct2_processed)\n",
    "    print('number of columns: ', len(data_processed.columns))\n",
    "    print(\"✅ DATA PROCESSED WITH SHAPE:\", data_processed.shape)\n",
    "\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_features_v2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4f235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_data/data_cleaned_and_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd38f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by='f_ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.query(\"f_pm_01m < 15 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e825154",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e557a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train.iloc[:24882]\n",
    "# X_val = X.iloc[24882:49544]\n",
    "# X_test = X.iloc[49544:]\n",
    "# y_train = y.iloc[:24882]\n",
    "# y_val = y.iloc[24882:49544]\n",
    "# y_test = y.iloc[49544:]\n",
    "# backtest_train = backtest.iloc[:24882]\n",
    "# backtest_val = backtest.iloc[24882:49544]\n",
    "# backtest_test = backtest.iloc[49544:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data chronologically to avoid look-ahead bias:\n",
    "test_start_date = '2022-08-20 18:45:00'  # max_date - 1 yr\n",
    "val_start_date = '2022-02-20 18:45:00'  # max_date - 1.5 yrs\n",
    "# Split the data into test and train\n",
    "train = data[data['f_ko'] <= val_start_date]\n",
    "val = data[(data['f_ko'] > val_start_date) & (data['f_ko'] <= test_start_date)]\n",
    "test = data[data['f_ko'] > test_start_date]\n",
    "# Split the data into X and y\n",
    "X_train = train.drop(columns=[\"f_place\", \"f_pm_15m\", \"f_pm_05m\", \"f_pm_01m\", \"f_pm_15m_p_back\", \"f_id\", \"f_ko\"])\n",
    "X_val = val.drop(columns = [\"f_place\", \"f_pm_15m\", \"f_pm_05m\", \"f_pm_01m\", \"f_pm_15m_p_back\", \"f_id\", \"f_ko\"])\n",
    "X_test = test.drop(columns = [\"f_place\", \"f_pm_15m\", \"f_pm_05m\", \"f_pm_01m\", \"f_pm_15m_p_back\", \"f_id\", \"f_ko\"])\n",
    "y_train = train[\"f_place\"]\n",
    "y_val = val[\"f_place\"]\n",
    "y_test = test[\"f_place\"]\n",
    "len(X_train), len(X_val), len(X_test), len(y_train), len(y_val), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe51f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting= data[data['f_ko'] > test_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299edc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=X.iloc[:73753]\n",
    "# X_val=X.iloc[73753:91432]\n",
    "# X_test=X.iloc[91432:]\n",
    "# y_train=y.iloc[:73753]\n",
    "# y_val=y.iloc[73753:91432]\n",
    "# y_test=y.iloc[91432:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA()\n",
    "\n",
    "# # Fit PCA on your data\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# # Get explained variance ratio\n",
    "# explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# # Calculate cumulative explained variance\n",
    "# cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# # Plot explained variance ratio\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "# plt.title('Cumulative Explained Variance')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea300cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(implied_probabilities, bet_amount, threshold=0.06):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # Calculate the potential profit/loss for each prediction\n",
    "#         bet_placed = y_pred > (implied_probabilities + threshold)\n",
    "#         profit_loss = tf.where(y_true == 1, tf.where(bet_placed, (implied_probabilities-1) * bet_amount * y_pred, 0)\n",
    "#                                , tf.where(bet_placed,bet_amount * y_pred, 0))\n",
    "\n",
    "#         # Calculate the binary cross-entropy loss\n",
    "#         binary_crossentropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "\n",
    "#         # Combine all loss components\n",
    "#         custom_loss_value = tf.reduce_mean(binary_crossentropy + profit_loss)\n",
    "\n",
    "#         return custom_loss_value\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.01))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.00005), loss=\"binary_crossentropy\", metrics=[Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define callbacks (optional)\n",
    "early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pca, y_train, batch_size=16, epochs=1000, validation_data=(X_val_pca, y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaec012",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86715701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred, bins=50, kde=True);  # Adjust the number of bins as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c82a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backtesting[\"y_pred\"]=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"probabilities\"]=1/backtesting[\"f_pm_15m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"diff\"] = backtesting[\"y_pred\"] - backtesting[\"probabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593301ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(backtesting[\"diff\"], bins=20, kde=True); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"bet_0.1\"] = backtesting.apply(lambda row: 1 if row['y_pred'] - row['probabilities'] > 0.1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"bet_0.05\"] = backtesting.apply(lambda row: 1 if row['y_pred'] - row['probabilities'] > 0.05 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28481b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"bet_0.15\"] = backtesting.apply(lambda row: 1 if row['y_pred'] - row['probabilities'] > 0.15 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"bet_0.05\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9edc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting[\"bet_0.1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['bet_0.15'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['bets_placed_0.15'] = backtesting['bet_0.15'].cumsum()\n",
    "backtesting['bets_placed_0.1'] = backtesting[\"bet_0.1\"].cumsum()\n",
    "backtesting['bets_placed_0.05'] = backtesting['bet_0.05'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['profit_0.05']=backtesting['bet_0.05'] * backtesting[\"f_pm_15m_p_back\"]\n",
    "backtesting['profit_0.1']=backtesting['bet_0.1'] * backtesting[\"f_pm_15m_p_back\"]\n",
    "backtesting['profit_0.15']=backtesting['bet_0.15'] * backtesting[\"f_pm_15m_p_back\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174012e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size (width, height)\n",
    "plt.plot(backtesting['bets_placed_0.05'], backtesting['profit_0.05'].cumsum(), linestyle='-', color='b', label='5% threshold')\n",
    "plt.plot(backtesting['bets_placed_0.1'], backtesting['profit_0.1'].cumsum(), linestyle='-', color='r', label='10% threshold')\n",
    "plt.plot(backtesting['bets_placed_0.15'], backtesting['profit_0.15'].cumsum(), linestyle='-', color='g', label='15% threshold')\n",
    "plt.title('Profit Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Profit')\n",
    "plt.grid(True)\n",
    "\n",
    "# tick_interval = pd.DateOffset(days=30)  \n",
    "# start_date = backtesting['f_ko'].min()\n",
    "# end_date = backtesting['f_ko'].max()\n",
    "# x_ticks = pd.date_range(start=start_date, end=end_date, freq=tick_interval)\n",
    "# plt.xticks(x_ticks, x_ticks.strftime('%b %d')) \n",
    "\n",
    "# plt.xticks(rotation=45)  \n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ef9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['profit_0.05'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d97c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['profit_0.1'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b98fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting['profit_0.15'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"model_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266d52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63420648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a5c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['f_ko'] = X_test['f_ko'].astype(int) // 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c72693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['f_ko'] = pd.to_datetime(X_train['f_ko'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['f_ko']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['f_ko'] = scaler.fit_transform(X_test[['f_ko']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['f_ko']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67be082",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf7bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
